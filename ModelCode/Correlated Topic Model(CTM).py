# -*- coding: utf-8 -*-
"""POS Tagging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k7qztuxaWCcLCYhOaKNac4kEL_wCNAV1
"""

import os
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.corpora import Dictionary
from gensim.models import TfidfModel, HdpModel
from gensim.models.ldamulticore import LdaMulticore
from scipy.sparse import csr_matrix
from sklearn.preprocessing import normalize

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt_tab')

class CorrelatedTopicModel:
    def __init__(self, folder_path='Doc', num_topics=50, passes=300):
        """
        Initialize Correlated Topic Model

        :param folder_path: Path to folder containing documents
        :param num_topics: Number of topics to extract
        :param passes: Number of passes through the corpus
        """
        self.folder_path = folder_path
        self.num_topics = num_topics
        self.passes = passes

        # Preprocessing setup
        self.stop_words = set(stopwords.words('english'))
        self.downweight_words = {"assessment", "graded", "credit", "requires", "cmkl"}

        # Storage for processed data
        self.documents = []
        self.competency_labels = []
        self.dictionary = None
        self.corpus_bow = None
        self.corpus_tfidf = None
        self.ctm_model = None

    def preprocess(self, text):
        """
        Preprocess text by tokenizing and removing stop words

        :param text: Input text
        :return: List of preprocessed tokens
        """
        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
        tokens = [word for word in tokens if word.isalpha() and word not in self.stop_words]
        return tokens

    def load_documents(self):
        """
        Load and preprocess documents from the specified folder
        """
        for filename in os.listdir(self.folder_path):
            file_path = os.path.join(self.folder_path, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                # Extract competency name from the first line
                first_line = f.readline().strip()
                competency_name = re.findall(r'\((.*?)\)', first_line)
                self.competency_labels.append(competency_name[0] if competency_name else "Unknown")

                text = f.read()
                self.documents.append(self.preprocess(text))

        # Create dictionary and corpus
        self.dictionary = Dictionary(self.documents)

        # Down-weight specific words
        for word in self.downweight_words:
            if word in self.dictionary.token2id:
                token_id = self.dictionary.token2id[word]
                self.dictionary.dfs[token_id] *= 0.1  # Down-weight by 90%

        # Convert to bag of words and TF-IDF
        self.corpus_bow = [self.dictionary.doc2bow(doc) for doc in self.documents]
        tfidf_model = TfidfModel(self.corpus_bow)
        self.corpus_tfidf = tfidf_model[self.corpus_bow]

    def train_ctm(self):
        """
        Train Correlated Topic Model using an approximation method
        """
        # Use LDA as a base for topic correlation
        lda_model = LdaMulticore(
            corpus=self.corpus_tfidf,
            id2word=self.dictionary,
            num_topics=self.num_topics,
            passes=self.passes
        )

        # Extract topic-word and document-topic matrices
        # Topic-word matrix
        topic_word_matrix = np.zeros((self.num_topics, len(self.dictionary)))
        for topic_id in range(self.num_topics):
            for word_id, prob in lda_model.get_topic_terms(topic_id):
                topic_word_matrix[topic_id, word_id] = prob

        # Document-topic matrix
        doc_topic_matrix = np.zeros((len(self.documents), self.num_topics))
        for doc_idx, doc in enumerate(self.corpus_tfidf):
            doc_topics = lda_model.get_document_topics(doc)
            for topic_id, prob in doc_topics:
                doc_topic_matrix[doc_idx, topic_id] = prob

        # Normalize matrices
        topic_word_matrix = normalize(topic_word_matrix, axis=1, norm='l1')
        doc_topic_matrix = normalize(doc_topic_matrix, axis=1, norm='l1')

        # Compute topic correlation matrix
        topic_correlation = np.corrcoef(doc_topic_matrix.T)

        # Store the model components
        self.ctm_model = {
            'topic_word_matrix': topic_word_matrix,
            'doc_topic_matrix': doc_topic_matrix,
            'topic_correlation': topic_correlation
        }

    def analyze_topics(self):
        """
        Analyze and print topic information
        """
        if self.ctm_model is None:
            raise ValueError("Model not trained. Call train_ctm() first.")

        # Print top words for each topic
        print("Top Words for Each Topic:")
        topic_word_matrix = self.ctm_model['topic_word_matrix']
        for topic_idx in range(self.num_topics):
            # Get top 10 words for the topic
            top_words_indices = np.argsort(topic_word_matrix[topic_idx])[-10:][::-1]
            top_words = [self.dictionary[idx] for idx in top_words_indices]
            print(f"Topic {topic_idx + 1}: {', '.join(top_words)}")

        # Print topic correlation
        print("\nTopic Correlation Matrix (Diagonal represents self-correlation):")
        np.set_printoptions(precision=2, suppress=True)
        print(self.ctm_model['topic_correlation'])

    def group_documents(self):
        """
        Group documents by their top topics
        """
        doc_topic_matrix = self.ctm_model['doc_topic_matrix']
        document_groups = {}

        for doc_idx, topic_dist in enumerate(doc_topic_matrix):
            # Get top 5 topics
            top_5_topic_ids = np.argsort(topic_dist)[-5:][::-1]

            # Create a tuple key for grouping
            group_key = tuple(top_5_topic_ids)

            # Group documents
            if group_key not in document_groups:
                document_groups[group_key] = []
            document_groups[group_key].append(f"Document {doc_idx + 1} (Original: {self.competency_labels[doc_idx]})")

        # Print document groups
        print("\nDocument Grouping by Top 5 Topics:")
        for topic_ids, docs in document_groups.items():
            print(f"\nGroup with Top 5 Topics {topic_ids}:")
            for doc in docs:
                print(f"  {doc}")

        return document_groups

    def run(self):
        """
        Run the complete Correlated Topic Model analysis
        """
        # Load documents
        self.load_documents()

        # Train the model
        self.train_ctm()

        # Analyze topics
        self.analyze_topics()

        # Group documents
        return self.group_documents()

# Main execution
if __name__ == "__main__":
    ctm = CorrelatedTopicModel()
    ctm.run()